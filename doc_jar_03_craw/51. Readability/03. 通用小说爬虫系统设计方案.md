<span  style="font-family: Simsun,serif; font-size: 17px; ">

# 通用小说爬虫系统设计方案：Java, Jsoup/Readability, MySQL, Redis

## 摘要

本方案旨在设计一个**高通用性**的小说爬虫系统，利用 Java 作为开发语言，**Jsoup/Readability 算法**实现通用内容提取，并以 *
*Redis** 作为高性能任务队列和去重机制，**MySQL** 作为持久化存储。核心设计理念是采用**生产者-消费者模型**，将爬虫对网站结构的依赖降到最低。

---

## 一、 架构概览与技术分工

系统采用分层解耦的架构，分为调度层、处理层和存储层。

| 组件                | 技术                      | 角色和职责                                                      |
|:------------------|:------------------------|:-----------------------------------------------------------|
| **爬虫引擎 (Engine)** | Java (如 Spring Boot)    | 负责 Worker 线程的调度、HTTP 请求管理、流控与异常处理。                         |
| **内容解析**          | Jsoup + 自定义 Readability | **实现通用性**。解析 HTML，利用 Readability 评分机制提取纯净的正文、标题、章节列表和简介。   |
| **任务队列/缓存**       | Redis                   | **高并发与去重**。作为 URL 待爬队列 (List)，URL 已爬记录 (Set)，以及限速器 (Hash)。 |
| **持久化存储**         | MySQL                   | **结构化归档**。存储小说元数据和章节内容（LONGTEXT）。                          |

---

## 二、 Redis 模块设计：任务调度与去重

Redis 用于高效管理爬取任务和防止重复劳动。

| Redis 数据结构      | Key 示例                      | 作用                                                    |
|:----------------|:----------------------------|:------------------------------------------------------|
| **List (队列)**   | `aiReadabilityNovel:queue:todo`          | **任务队列**。存储待爬取的章节 URL，Worker 使用 `LPOP` 获取任务。          |
| **Set (集合)**    | `aiReadabilityNovel:url:crawled`         | **去重机制**。存储所有已爬取 URL 的哈希值，通过 `SISMEMBER` 快速检查，避免重复抓取。 |
| **Hash/String** | `aiReadabilityNovel:rate_limit:{domain}` | **限速器**。存储上次访问某个域名的时间戳，用于控制爬取频率，遵守网站的爬虫礼仪。            |
| **Hash**        | `aiReadabilityNovel:metadata:{novel_id}` | **热数据缓存**。存储小说最新章节 URL 或状态，供快速查询。                     |

---

## 三、 MySQL 数据结构设计

核心数据模型包括小说元数据和章节内容。

### 1. `aiReadabilityNovel` 表（小说元数据）

| 字段名           | 类型           | 描述                 |
|:--------------|:-------------|:-------------------|
| `novel_id`    | BIGINT (PK)  | 小说唯一 ID            |
| `title`       | VARCHAR(255) | 小说标题               |
| `author`      | VARCHAR(100) | 作者                 |
| `synopsis`    | TEXT         | 小说简介/概要 (需启发式提取)   |
| `catalog_url` | VARCHAR(512) | 原始目录页 URL (用于更新追踪) |
| `status`      | VARCHAR(20)  | 连载状态（连载中/已完结）      |

### 2. `aiReadabilityChapter` 表（章节内容）

| 字段名             | 类型           | 描述                           |
|:----------------|:-------------|:-----------------------------|
| `chapter_id`    | BIGINT (PK)  | 章节唯一 ID                      |
| `novel_id`      | BIGINT (FK)  | 外键，关联到 `aiReadabilityNovel` 表             |
| `chapter_index` | INT          | 章节序号 (确保章节顺序)                |
| `chapter_title` | VARCHAR(255) | 章节标题                         |
| `content`       | LONGTEXT     | **章节纯文本内容 (Readability 提取)** |
| `source_url`    | VARCHAR(512) | 原始章节页 URL                    |

---

## 四、 爬虫核心工作流程 (Worker Service)

Worker 线程负责执行从 Redis 取出任务到存入 MySQL 的整个过程。

### 阶段 A：目录页解析（Metadata & 章节链接）

1. **任务获取：** 从 Redis `aiReadabilityNovel:queue:todo` 中取出目录页 URL。
2. **Metadata 提取：**
   * 提取小说标题、作者、简介等（通过 `title` 标签和 ID/Class 启发式规则）。
   * 将元数据保存至 MySQL `aiReadabilityNovel` 表，获取 `novel_id`。
3. **章节列表提取（通用化）：**
   * **关键步骤（改进版）：**
      - 仅统计同域链接，文本长度在 2~60 之间。
      - **策略1（优先）：** 通过关键词查找章节列表容器
         - 查找包含"最新章节"或"正文"关键词的元素，向上查找父容器
         - 排除包含"推荐阅读"或"推荐地址"的容器
         - 找到包含至少5个有效链接的容器作为章节列表
      - **策略2（备选）：** 通过链接数量查找容器
         - 找出链接数量最多的父容器
         - 排除推荐阅读区域的链接和容器（通过检查是否包含"推荐阅读"或"推荐地址"关键词）
      - **策略3（兜底）：** 如果前两个策略都未找到
         - 按页面顺序截取前200个合格链接
         - 同样排除推荐阅读区域的链接
      - 生成章节链接列表时使用 HashSet 去重，chapterIndex 按页面出现顺序递增。
   * 将所有识别出的章节 URL，连同 `novel_id`，推入 Redis 队列。
4. **目录翻页探测（逻辑一）：**
   * 如果目录页存在“下一页/下一章列表”等翻页链接，使用分页探测器继续抓取后续目录页，补全章节链接列表。
   * 翻页后的目录页与初始目录页同等处理，直至无更多分页。

### 阶段 B：章节页解析（Readability 内容提取）

1. **任务获取：** 从 Redis 队列中取出章节 URL。
2. **限速检查：** 检查 Redis 限速器，确保遵守网站访问频率限制。
3. **内容提取 (Readability 核心)：**
   * 对章节页 HTML 执行 **Readability 算法**（基于 Jsoup 的评分和清理逻辑）。
   * **产出：** 纯净的 `chapter_title` 和 `content`。
4. **翻页链接探测（逻辑二：内容页翻页）：**
   * 使用**翻页探测器**查找链接文本包含“下一章 / 下一页 / Next”等关键词的 `<a>` 标签，或 `link[rel=next]`。
   * 如果找到未抓取的 URL，则推回 Redis 章节队列，实现**自动连续阅读**（章节链式抓取）。

### 阶段 C（可选）：接口/编排层

1. **同步编排（示例）：**
   * 输入目录 URL → 拉取目录（含目录分页） → 依次拉取章节正文（含正文翻页） → 持久化 Novel/Chapter。
2. **接口示例：**
   * `POST /api/crawl/catalog`，入参包含目录 URL、是否跟踪目录分页、是否跟踪正文翻页、翻页最大深度。
3. **组件：**
   * `CatalogPaginationDetector`：目录分页探测。
   * `NextPageDetector`：正文翻页探测。
   * `CrawlOrchestrator`：串联目录解析、翻页探测、Readability、持久化。
5. **数据持久化：** 将提取出的数据写入 MySQL `aiReadabilityChapter` 表。

---

## 五、 反爬与稳定性（可选开关）

- **IP/代理池（可选）**：通过配置开关启用或禁用代理池；默认关闭，低访问量场景直接走本机出口。需要时可支持轮换代理与健康检查。
- **UA/请求头随机化**：多 User-Agent 轮换，适度添加 Accept-Language/Referer 等头部，避免简单封锁。
- **节流与重试**：按域名限速（Hash 记录上次访问时间）、指数退避重试、失败任务回退队列；设置超时与最大重定向。
- **容错与监控**：记录抓取/解析错误原因，统计成功率与延迟，关注 Redis 队列堆积。可使用 Prometheus/Grafana 但非必选。
- **robots.txt**：本方案不遵循 robots.txt，需在上线前评估合规风险。

---

## 六、 通用性与维护性总结

* **高通用性：** 依赖 Readability 算法实现内容与章节标题的通用提取，极大地减少了对 XPath/CSS Selector 的依赖。
* **高弹性：** 任务调度完全由 Redis 管理，可随时增加或移除 Java Worker 实例，方便弹性伸缩。
* **维护策略：** 只有少数结构极特殊的网站需要单独配置**解析策略（Strategy Pattern）**，绝大部分网站可保持通用配置。

---

## 七、 TODO：通用/站点级去噪策略（后续落地）

> 待实现：将正文去噪拆为“通用规则 + 站点特定规则”两级，便于降低重复和噪声混入。

- 通用去噪（适用于大部分站点）：
   - 过滤导航/翻页/当前位置类文本（如“当前位置”“上一章”“下一章”“目录”“首页”“手机阅读”）。
   - 过滤站点水印/版权提示（如“有度中文网”等可配置关键字）。
   - 段落长度下限、标题重复过滤、归一化去重（去空白/标点）。
- 站点特定规则（按域名配置）：
   - 针对特定站点的页脚广告、固定口号、独有水印做额外关键字/正则过滤。
   - 可扩展为策略模式：不同站点提供自定义选择器或附加黑名单。
- 为什么需要两级：
   - 纯文本密度算法难以区分高密度广告/导航，通用规则提供安全网。
   - 不同站点模板差异大，通过可配置的域名级规则减轻误判、减少重复。

（本节为规划，暂不开发，后续按优先级落地。）

---

### TODO：正文去噪配置化（待实现）

- 公共去噪规则：适用于大部分站点的导航/版权/上一章下一章等关键词过滤、长度阈值、段落去重。
- 站点专属规则：按域名配置特定关键词或正则，清理站点水印、模板口号、特殊广告段落。
- 组合策略：先执行通用规则，再按域名执行站点规则，最终由算法评分/文本密度选择正文容器。
- 后续可抽象为 ReadabilityFilter 接口 + 策略工厂，按域名选择不同过滤链。

---

### TODO：章节自动翻页/连续阅读（待实现）

- 目标：从目录入队后，章节页内可自动识别“下一章/下一页”链接，持续入队，直至无更多章节。
- 组件：
   - `NextPageDetector`：抽象翻页探测接口，默认实现通过关键词与 `rel=next` 选择器查找下一章 URL。
   - Worker 流程：章节 Worker 抓取→Readability→持久化→`NextPageDetector` 查找→未抓取的 URL 入章节队列。
- 去重与限速：入队前检查 `aiReadabilityNovel:url:crawled`，按域名限速；失败重试与回退队列。
- 配置化：可为特定站点追加自定义翻页关键词或选择器，降低漏检/误判。

---

### TODO：翻页探测拆分（目录分页 vs 内容分页）

- 目录分页（逻辑一）：目录页存在分页导航（下一页/下一章列表/页码）。需在目录 Worker 中重复抓取并合并章节链接。
- 内容分页（逻辑二）：章节正文分多页或存在“下一章”链接。由章节 Worker 使用翻页探测器将下一页/下一章 URL 入队。
- 组件规划：
   - `CatalogPaginationDetector`：识别目录页的下一页链接（关键词、分页导航选择器、rel=next）。
   - `NextPageDetector`：用于正文翻页，支持 rel=next 与“下一章/下一页/next”关键词。
- 去重与限速：翻页入队前检查 `aiReadabilityNovel:url:crawled`，按域名限速，失败回退队列。

### TODO：目录分析有问题

-

</span>